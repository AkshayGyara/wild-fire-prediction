{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers. normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "from numpy import argmax\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/Users/nithyakuchadi/Downloads/Dataset/'\n",
    "train_data=[]\n",
    "train_label=[]\n",
    "test_data=[]\n",
    "IMG_SIZE=950\n",
    "heights = []\n",
    "widths = []\n",
    "img_data_list=[]\n",
    "labels_list = []\n",
    "labels_name={'fire':0,'smoke':1,'nofire':2}\n",
    "def label_img(word_label):\n",
    "    if (word_label == 'fire') : return np.array([1,0,0])\n",
    "    elif (word_label == 'smoke') : return np.array([0,1,0])\n",
    "    elif (word_label == 'no fire') or (word_label == 'nofire'): return np.array([0,0,1])\n",
    "    else : return np.array([0,0,1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Height: 950.0\n",
      "Max Height: 950\n",
      "Min Height: 950\n",
      "\n",
      "\n",
      "Average Width: 950.0\n",
      "Max Width: 950\n",
      "Min Width: 950\n",
      "(1492, 950, 950)\n"
     ]
    }
   ],
   "source": [
    "for root,folders,files in os.walk(DIR):\n",
    "    if len(files) < 2: continue\n",
    "    image_file = os.path.join(root, files[3])\n",
    "    label_name = open(os.path.join(root, files[1])).readlines()[1].strip(\"\\n\").lower()\n",
    "    img=Image.open(image_file)\n",
    "    input_img=cv2.imread(image_file)\n",
    "    input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "    input_img_resize=cv2.resize(input_img,(IMG_SIZE,IMG_SIZE))\n",
    "    img = img.convert('L')\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.ANTIALIAS)\n",
    "    data=np.array(img)\n",
    "    train_data.append(data)\n",
    "    heights.append(data.shape[0])\n",
    "    widths.append(data.shape[1])\n",
    "    trainLabels=label_img(label_name)\n",
    "    train_label.append(trainLabels)\n",
    "    img_data_list.append(input_img_resize)\n",
    "    labels_list.append(label_name)\n",
    "    \n",
    "avg_height = sum(heights) / len(heights)\n",
    "avg_width = sum(widths) / len(widths)\n",
    "print(\"Average Height: \" + str(avg_height))\n",
    "print(\"Max Height: \" + str(max(heights)))\n",
    "print(\"Min Height: \" + str(min(heights)))\n",
    "print('\\n')\n",
    "print(\"Average Width: \" + str(avg_width))\n",
    "print(\"Max Width: \" + str(max(widths)))\n",
    "print(\"Min Width: \" + str(min(widths)))\n",
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print (img_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['d', 'fire', 'no fire', 'nofire', 'smoke'], dtype='<U7'), array([  3, 547, 108, 305, 529]))\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(labels_list)\n",
    "print(np.unique(labels,return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = np.array(train_data).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "trainLabels = np.array(train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(trainImages, trainLabels, random_state = 20, \n",
    "                                                   test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = np.mean(train_x, axis=0)\n",
    "train_std = np.std(train_x, axis=0)\n",
    "test_mean = np.mean(test_x, axis=0)\n",
    "test_std = np.std(test_x, axis=0)\n",
    "\n",
    "train_norm = (train_x - train_mean) / train_std\n",
    "test_norm = (test_x - test_mean) / test_std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=950\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 948, 948, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 474, 474, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 474, 474, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 472, 472, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 236, 236, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 236, 236, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 234, 234, 96)      55392     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 117, 117, 96)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 117, 117, 96)      384       \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 115, 115, 96)      83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 57, 57, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 57, 57, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 55, 55, 96)        83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 27, 27, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 25, 25, 96)        83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 12, 12, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 12, 12, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 10, 10, 96)        83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 5, 5, 96)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 5, 5, 96)          384       \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 3, 3, 96)          83040     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 1, 1, 96)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 1, 1, 96)          384       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 96)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               24832     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 550,211\n",
      "Trainable params: 548,867\n",
      "Non-trainable params: 1,344\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=[\"accuracy\"])\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='rmsprop',metrics=[\"accuracy\"])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 895 samples, validate on 224 samples\n",
      "Epoch 1/50\n",
      "895/895 [==============================] - 1554s 2s/step - loss: 0.6322 - acc: 0.6641 - val_loss: 1.3041 - val_acc: 0.6592\n",
      "Epoch 2/50\n",
      "895/895 [==============================] - 1799s 2s/step - loss: 0.5948 - acc: 0.6886 - val_loss: 0.5737 - val_acc: 0.7098\n",
      "Epoch 3/50\n",
      "895/895 [==============================] - 1736s 2s/step - loss: 0.5731 - acc: 0.7095 - val_loss: 0.5888 - val_acc: 0.7202\n",
      "Epoch 4/50\n",
      "895/895 [==============================] - 1742s 2s/step - loss: 0.5536 - acc: 0.7069 - val_loss: 0.5490 - val_acc: 0.7247\n",
      "Epoch 5/50\n",
      "895/895 [==============================] - 1688s 2s/step - loss: 0.5399 - acc: 0.7099 - val_loss: 0.5699 - val_acc: 0.7217\n",
      "Epoch 6/50\n",
      "895/895 [==============================] - 1659s 2s/step - loss: 0.5186 - acc: 0.7304 - val_loss: 0.5072 - val_acc: 0.7530\n",
      "Epoch 7/50\n",
      "895/895 [==============================] - 1700s 2s/step - loss: 0.5191 - acc: 0.7236 - val_loss: 0.5605 - val_acc: 0.7396\n",
      "Epoch 8/50\n",
      "895/895 [==============================] - 1693s 2s/step - loss: 0.5263 - acc: 0.7292 - val_loss: 0.6299 - val_acc: 0.6890\n",
      "Epoch 9/50\n",
      "895/895 [==============================] - 1684s 2s/step - loss: 0.5073 - acc: 0.7453 - val_loss: 0.5246 - val_acc: 0.7426\n",
      "Epoch 10/50\n",
      "895/895 [==============================] - 1672s 2s/step - loss: 0.5097 - acc: 0.7408 - val_loss: 0.5532 - val_acc: 0.7247\n",
      "Epoch 11/50\n",
      "895/895 [==============================] - 1726s 2s/step - loss: 0.4960 - acc: 0.7453 - val_loss: 0.5744 - val_acc: 0.7321\n",
      "Epoch 12/50\n",
      "895/895 [==============================] - 1710s 2s/step - loss: 0.4922 - acc: 0.7508 - val_loss: 0.5183 - val_acc: 0.7292\n",
      "Epoch 13/50\n",
      "895/895 [==============================] - 1684s 2s/step - loss: 0.4835 - acc: 0.7572 - val_loss: 0.5503 - val_acc: 0.7232\n",
      "Epoch 14/50\n",
      "895/895 [==============================] - 1623s 2s/step - loss: 0.4695 - acc: 0.7646 - val_loss: 0.5323 - val_acc: 0.7277\n",
      "Epoch 15/50\n",
      "895/895 [==============================] - 1753s 2s/step - loss: 0.4654 - acc: 0.7665 - val_loss: 0.5199 - val_acc: 0.7321\n",
      "Epoch 16/50\n",
      "895/895 [==============================] - 1737s 2s/step - loss: 0.4606 - acc: 0.7657 - val_loss: 0.5680 - val_acc: 0.7485\n",
      "Epoch 17/50\n",
      "895/895 [==============================] - 1747s 2s/step - loss: 0.4492 - acc: 0.7724 - val_loss: 0.6376 - val_acc: 0.7113\n",
      "Epoch 18/50\n",
      "895/895 [==============================] - 1739s 2s/step - loss: 0.4542 - acc: 0.7736 - val_loss: 0.5270 - val_acc: 0.7574\n",
      "Epoch 19/50\n",
      "895/895 [==============================] - 1721s 2s/step - loss: 0.4508 - acc: 0.7754 - val_loss: 0.5331 - val_acc: 0.7485\n",
      "Epoch 20/50\n",
      "895/895 [==============================] - 1985s 2s/step - loss: 0.4248 - acc: 0.7866 - val_loss: 0.5306 - val_acc: 0.7619\n",
      "Epoch 21/50\n",
      "895/895 [==============================] - 1714s 2s/step - loss: 0.4336 - acc: 0.7732 - val_loss: 0.5182 - val_acc: 0.7619\n",
      "Epoch 22/50\n",
      "895/895 [==============================] - 1662s 2s/step - loss: 0.4338 - acc: 0.7877 - val_loss: 0.5247 - val_acc: 0.7232\n",
      "Epoch 23/50\n",
      "895/895 [==============================] - 1696s 2s/step - loss: 0.4223 - acc: 0.7888 - val_loss: 0.5252 - val_acc: 0.7560\n",
      "Epoch 24/50\n",
      "895/895 [==============================] - 1717s 2s/step - loss: 0.4221 - acc: 0.7829 - val_loss: 0.5504 - val_acc: 0.7500\n",
      "Epoch 25/50\n",
      "895/895 [==============================] - 1549s 2s/step - loss: 0.4152 - acc: 0.7944 - val_loss: 0.5215 - val_acc: 0.7634\n",
      "Epoch 26/50\n",
      "895/895 [==============================] - 1569s 2s/step - loss: 0.4147 - acc: 0.7855 - val_loss: 0.5299 - val_acc: 0.7693\n",
      "Epoch 27/50\n",
      "895/895 [==============================] - 1531s 2s/step - loss: 0.4094 - acc: 0.7885 - val_loss: 0.5647 - val_acc: 0.7634\n",
      "Epoch 28/50\n",
      "895/895 [==============================] - 1584s 2s/step - loss: 0.3814 - acc: 0.8123 - val_loss: 0.5358 - val_acc: 0.7500\n",
      "Epoch 29/50\n",
      "895/895 [==============================] - 1580s 2s/step - loss: 0.3949 - acc: 0.8041 - val_loss: 0.5507 - val_acc: 0.7336\n",
      "Epoch 30/50\n",
      "895/895 [==============================] - 1551s 2s/step - loss: 0.3725 - acc: 0.8164 - val_loss: 0.5772 - val_acc: 0.7708\n",
      "Epoch 31/50\n",
      "895/895 [==============================] - 1583s 2s/step - loss: 0.3802 - acc: 0.8089 - val_loss: 0.5701 - val_acc: 0.7485\n",
      "Epoch 32/50\n",
      "895/895 [==============================] - 1944s 2s/step - loss: 0.3618 - acc: 0.8138 - val_loss: 0.5301 - val_acc: 0.7649\n",
      "Epoch 33/50\n",
      "895/895 [==============================] - 1892s 2s/step - loss: 0.3609 - acc: 0.8145 - val_loss: 0.5636 - val_acc: 0.7440\n",
      "Epoch 34/50\n",
      "895/895 [==============================] - 2054s 2s/step - loss: 0.3731 - acc: 0.8175 - val_loss: 0.5304 - val_acc: 0.7887\n",
      "Epoch 35/50\n",
      "895/895 [==============================] - 1919s 2s/step - loss: 0.3684 - acc: 0.8145 - val_loss: 0.5654 - val_acc: 0.7351\n",
      "Epoch 36/50\n",
      "895/895 [==============================] - 1699s 2s/step - loss: 0.3702 - acc: 0.8063 - val_loss: 0.5475 - val_acc: 0.7455\n",
      "Epoch 37/50\n",
      "895/895 [==============================] - 1645s 2s/step - loss: 0.3680 - acc: 0.8134 - val_loss: 0.5329 - val_acc: 0.7589\n",
      "Epoch 38/50\n",
      "895/895 [==============================] - 1841s 2s/step - loss: 0.3542 - acc: 0.8231 - val_loss: 0.5262 - val_acc: 0.7693\n",
      "Epoch 39/50\n",
      "895/895 [==============================] - 1717s 2s/step - loss: 0.3546 - acc: 0.8235 - val_loss: 0.5722 - val_acc: 0.7515\n",
      "Epoch 40/50\n",
      "895/895 [==============================] - 1744s 2s/step - loss: 0.3340 - acc: 0.8365 - val_loss: 0.5636 - val_acc: 0.7813\n",
      "Epoch 41/50\n",
      "400/895 [============>.................] - ETA: 14:45 - loss: 0.3388 - acc: 0.8175"
     ]
    }
   ],
   "source": [
    "history=model.fit(train_norm,train_y,validation_split=0.2,batch_size = 20, epochs = 40, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers. normalization import BatchNormalization\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "from numpy import argmax\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "seed = 1000\n",
    "def plot_history(history):\n",
    "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
    "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
    "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
    "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
    "    \n",
    "    if len(loss_list) == 0:\n",
    "        print('Loss is missing in history')\n",
    "        return \n",
    "    \n",
    "    ## As loss always exists\n",
    "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
    "    \n",
    "    ## Loss\n",
    "    plt.figure(1)\n",
    "    for l in loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    for l in val_loss_list:\n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    ## Accuracy\n",
    "    plt.figure(2)\n",
    "    for l in acc_list:\n",
    "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "    for l in val_acc_list:    \n",
    "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_history(history)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_norm, test_y,show_accuracy=True, verbose = 0)\n",
    "print(loss)\n",
    "print(acc * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = test_norm[0:1]\n",
    "print (test_image.shape)\n",
    "\n",
    "model_out=model.predict(test_image)\n",
    "print(model_out)\n",
    "print(np.argmax(datum))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_pred=model.predict(test_norm, batch_size=20, verbose=2)\n",
    "report = classification_report(test_y, y_pred.round())\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def draw_confusion_matrix(true,preds):\n",
    "    conf_matx = confusion_matrix(true, preds)\n",
    "    sns.heatmap(conf_matx, annot=True,annot_kws={\"size\": 12},fmt='g', cbar=False, cmap=\"viridis\")\n",
    "    plt.show()\n",
    "draw_confusion_matrix(test_y.argmax(axis=1), y_pred.round().argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
